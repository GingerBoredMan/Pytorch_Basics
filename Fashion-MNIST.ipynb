{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb3c466a550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784,256)\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128,64)\n",
    "        self.fc4 = nn.Linear(64,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim = 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 478.5995887070894)\n",
      "Training loss: 366.0257857963443)\n",
      "Training loss: 332.8351129963994)\n",
      "Training loss: 312.95698219537735)\n",
      "Training loss: 290.4961412027478)\n",
      "Training loss: 285.45220621675253)\n",
      "Training loss: 271.02334631979465)\n",
      "Training loss: 263.56902781128883)\n",
      "Training loss: 256.0787122771144)\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.view(images.shape[0],-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model.forward(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss})\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "\n",
    "\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "img = img.resize_(1, 784)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "\n",
    "\n",
    "ps = F.softmax(logits,dim = 1)\n",
    "\n",
    "helper.view_classify(img.resize_(1, 28, 28), ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
